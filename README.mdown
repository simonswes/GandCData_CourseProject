#Code Book

##Running run_analysis.r:
	*8Working Directory:** Working directory must be set to "./getdata_projectfiles_UCIHAR Dataset/UCI HAR Dataset"
	**Dir()** = [1] "activity_labels.txt" "features.txt"        "features_info.txt"   		"README.txt"          "test"                "train" 
	**Required Packages:** dplyr

##Process:
	run_analysis.r will transform the disparate pieces of data collected in the below study into a tidy data set that displays the mean for each observation, grouped first by activity and then by subject. The transofrmation involves piecing together the testing data, adding in the training data, and then renaming the columns and activities to create a user friendly data frame. The steps to achieve this are:
	
	1. Read the labels for the activies and the features into R
	2. Creating the testing1 data set by combining subject_test, X_test, and y_test
	3. Renaming the columns in testing1 to match the features
	4. Creating the training1 data set by combing subject_train, X_train, and y_train
	5. Renaming the columns in training1 to match the features
	6. Combinging the training1 and testing1 data sets.
	7. Renaming the activities to friendly names
	8. Loading the dplyr library
	9. Creating a data frame where the data is grouped by Subject and Activity
	10. Summarizing the data into a mean for each Activity by Subject

##Raw Data:
The Raw Data for this project was obtained by a team of researchers. More information about their research is here:

==================================================================
Human Activity Recognition Using Smartphones Dataset
Version 1.0
==================================================================
Jorge L. Reyes-Ortiz, Davide Anguita, Alessandro Ghio, Luca Oneto.
Smartlab - Non Linear Complex Systems Laboratory
DITEN - Universit√† degli Studi di Genova.
Via Opera Pia 11A, I-16145, Genoa, Italy.
activityrecognition@smartlab.ws
www.smartlab.ws
==================================================================

The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. 

The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain. See 'features_info.txt' for more details. 

License Information:
[1] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz, Spain. Dec 2012

